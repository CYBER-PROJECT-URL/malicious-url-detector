import pandas as pd
import numpy as np
import re
import joblib
from urllib.parse import urlparse
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, classification_report
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from transformers import AutoTokenizer, TFAutoModel

# --- 1. ×”×’×“×¨×•×ª ×’×œ×•×‘×œ×™×•×ª ---
# × ×ª×™×‘ ×œ×§×•×‘×¥ ×©×”×¢×œ×™×ª
DATASET_PATH = 'dataSet Kaggle.zip/Malicious URL v3.csv'
RANDOM_SEED = 42

# ×”×’×“×¨×•×ª BERT
BERT_MODEL_NAME = 'bert-base-uncased'
MAX_LEN = 128  # ××•×¨×š ××§×¡×™××œ×™ ×œ×¨×¦×£ (URL)
EMBEDDING_DIM = 768  # ×××“ ×”×™×™×¦×•×’ (Embedding) ×©×œ BERT-base


# --- 2. ×˜×¢×™× ×ª × ×ª×•× ×™× ×•×¢×™×‘×•×“ ××§×“×™× ---

def load_and_preprocess_data(path):
    """×˜×•×¢×Ÿ ××ª ×”× ×ª×•× ×™×, ×× ×§×” ×•×××™×¨ ××ª ×”×ª×•×•×™×ª ×œ×‘×™× ××¨×™×ª."""
    print(f"×˜×•×¢×Ÿ × ×ª×•× ×™× ×: {path}...")
    try:
        data = pd.read_csv(path, index_col=0)
    except Exception as e:
        print(f"×©×’×™××” ×‘×˜×¢×™× ×ª ×”×§×•×‘×¥: {e}")
        return None

    # × ×™×§×•×™: ×”×¡×¨×ª ×©×•×¨×•×ª ×¢× ×¢×¨×›×™ NULL ×‘'url' ××• 'type'
    data = data.dropna(subset=['url', 'type'])

    # ×”××¨×ª ×”×ª×•×•×™×ª (label) ×œ×‘×™× ××¨×™×ª:
    # benign (×‘×˜×•×—) -> 0
    # phishing, defacement, malware (×–×“×•× ×™) -> 1
    data['label'] = data['type'].apply(lambda x: 0 if x == 'benign' else 1)

    X = data['url'].astype(str)
    y = data['label']

    print(f"×¡×”\"×› ×“×•×’×××•×ª ×œ××—×¨ × ×™×§×•×™: {len(data)}")
    print(f"×—×œ×•×§×ª ×ª×•×•×™×•×ª: \n{data['label'].value_counts()}")
    return X, y


# --- 3. ××™×¦×•×™ ×ª×›×•× ×•×ª ×œ×§×¡×™×§×œ×™×•×ª (×œ-Random Forest) ---

def extract_lexical_features(url):
    """××—×œ×¥ ×ª×›×•× ×•×ª ××‘× ×™×•×ª ×•×œ×§×¡×™×§×œ×™×•×ª ××›×ª×•×‘×ª ×”××ª×¨ (×›××• ×‘××××¨ ×”×©× ×™)."""

    if not isinstance(url, str):
        url = ""

    features = {}
    parsed = urlparse(url)

    # 1. ××•×¨×š URL
    features['url_length'] = len(url)
    # 2. ××¡×¤×¨ ×”×ª×• '@' (××™× ×“×™×§×¦×™×” ×œ×”×¡×•×•××”)
    features['at_sign_count'] = url.count('@')
    # 3. × ×•×›×—×•×ª IP ×‘×›×ª×•×‘×ª ×”××ª×¨
    features['has_ip'] = 1 if re.search(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', parsed.netloc) else 0
    # 4. ××¡×¤×¨ ×¡×™×× ×™ '-'
    features['hyphens_count'] = url.count('-')
    # 5. ×¢×•××§ ×”× ×ª×™×‘ (××¡×¤×¨ '/')
    features['path_depth'] = url.count('/')
    # 6. ××•×¨×š ×”-Hostname
    features['hostname_length'] = len(parsed.netloc)
    # 7. × ×•×›×—×•×ª ××™×œ×•×ª ××¤×ª×— ×–×“×•× ×™×•×ª (×“×•×’××”)
    malicious_keywords = ['login', 'bank', 'secure', 'update', 'verify']
    features['malicious_keyword'] = sum(1 for kw in malicious_keywords if kw in url)
    # 8. × ×•×›×—×•×ª ×§×™×¦×•×¨ URL
    features['is_shortened'] = 1 if len(url) < 30 and 'bit.ly' in url or 'goo.gl' in url else 0

    return pd.Series(features)


# --- 4. ××™×¦×•×™ ×ª×›×•× ×•×ª ×¡×× ×˜×™×•×ª (×œ-Deep Learning - ×‘×”×©×¨××ª PMANet) ---

def get_bert_embeddings(urls):
    """××©×ª××© ×‘××•×“×œ BERT ×©××•××Ÿ ××¨××© ×›×“×™ ×œ×”×¤×™×§ ×™×™×¦×•×’×™× ×¡×× ×˜×™×™× (Embeddings)."""
    print(f"\n××¤×™×§ ×™×™×¦×•×’×™ BERT ×‘×××¦×¢×•×ª {BERT_MODEL_NAME}...")

    # ×˜×¢×™× ×ª ×˜×•×§× ×™×™×–×¨ ×•××•×“×œ
    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)
    model = TFAutoModel.from_pretrained(BERT_MODEL_NAME)

    # ×˜×•×§× ×™×–×¦×™×” ×©×œ ×›×œ ×›×ª×•×‘×•×ª ×”××ª×¨
    tokenized_inputs = tokenizer(
        urls.tolist(),
        max_length=MAX_LEN,
        truncation=True,
        padding='max_length',
        return_tensors='tf'
    )

    # ×—×™×©×•×‘ ×”×™×™×¦×•×’×™×
    # × ×“×¨×©×ª ×¡×‘×™×‘×ª Tensorflow
    try:
        with tf.device('/CPU:0'):  # ×©×™××•×© ×‘-CPU ×× ××™×Ÿ GPU ×–××™×Ÿ
            outputs = model(tokenized_inputs)
        # ×©×™××•×© ×‘-CLS token ×›×™×™×¦×•×’ ×©×œ ×›×œ ×”×¨×¦×£
        embeddings = outputs.last_hidden_state[:, 0, :].numpy()
    except Exception as e:
        print(f"×©×’×™××” ×‘×”×¤×¢×œ×ª ××•×“×œ BERT: {e}. ×‘×•×“×§ ×× TF2 ××•×’×“×¨ ×›×¨××•×™.")
        return None

    print(f"×”×¤×§×ª ×”×™×™×¦×•×’×™× ×”×¡×ª×™×™××”. ×¦×•×¨×”: {embeddings.shape}")
    return embeddings


# --- 5. ××™××•×Ÿ ××•×“×œ ×¨×©×ª × ×•×™×¨×•× ×™× (×”×—×œ×§ ×”×¢××•×§) ---

def create_nn_model(input_dim):
    """×‘× ×™×™×ª ×¨×©×ª × ×•×™×¨×•× ×™× ×¤×©×•×˜×” ×”××‘×•×¡×¡×ª ×¢×œ ×™×™×¦×•×’×™ BERT."""
    model = Sequential([
        Dense(512, activation='relu', input_shape=(input_dim,)),
        Dropout(0.5, seed=RANDOM_SEED),
        Dense(128, activation='relu'),
        Dropout(0.3, seed=RANDOM_SEED),
        Dense(1, activation='sigmoid')  # ×¤×œ×˜ ×‘×™× ××¨×™
    ])

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model


# --- 6. ×¤×•× ×§×¦×™×™×ª ×”××™××•×Ÿ ×”×¨××©×™×ª ---

def train_and_evaluate():
    # ×. ×˜×¢×™× ×ª × ×ª×•× ×™× ×•×¤×™×¦×•×œ
    X, y = load_and_preprocess_data(DATASET_PATH)
    if X is None:
        return

    # ×¤×™×¦×•×œ × ×ª×•× ×™× ×¢×‘×•×¨ ×©×ª×™ ×”×’×™×©×•×ª (×¦×¨×™×š ×œ×”×™×•×ª ×–×”×”)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y
    )

    # ×‘. ×”×›× ×ª × ×ª×•× ×™× ×œ-Random Forest (×ª×›×•× ×•×ª ×œ×§×¡×™×§×œ×™×•×ª)
    print("\n--- ×”×›× ×ª ×ª×›×•× ×•×ª ×œ×§×¡×™×§×œ×™×•×ª ---")
    X_lex_train = X_train.apply(extract_lexical_features)
    X_lex_test = X_test.apply(extract_lexical_features)

    # ×¡×§×™×™×œ×¨ (Normalization) ×œ×©×™×¤×•×¨ ×‘×™×¦×•×¢×™ Random Forest ×•-NN
    scaler = StandardScaler()
    X_lex_train_scaled = scaler.fit_transform(X_lex_train)
    X_lex_test_scaled = scaler.transform(X_lex_test)
    joblib.dump(scaler, 'scaler_lexical.joblib')
    print("×ª×›×•× ×•×ª ×œ×§×¡×™×§×œ×™×•×ª ×¢×•×‘×“×• ×•× ×©××¨×• ×¡×§×™×™×œ×¨.")

    # ×’. ×”×›× ×ª × ×ª×•× ×™× ×œ×¨×©×ª ×”× ×•×™×¨×•× ×™× (×ª×›×•× ×•×ª ×¡×× ×˜×™×•×ª - BERT)
    X_sem_train = get_bert_embeddings(X_train)
    X_sem_test = get_bert_embeddings(X_test)

    if X_sem_train is None:
        print("×œ× × ×™×ª×Ÿ ×œ×”××©×™×š ×œ××™××•×Ÿ ×”-NN ×œ×œ× ×™×™×¦×•×’×™ BERT.")
        X_sem_train = np.zeros((len(X_train), EMBEDDING_DIM))
        X_sem_test = np.zeros((len(X_test), EMBEDDING_DIM))

    # ×“. ××™××•×Ÿ ××•×“×œ Random Forest (ML ×§×œ××¡×™)
    print("\n--- ××™××•×Ÿ Random Forest (×œ×§×¡×™×§×œ×™) ---")
    rf_model = RandomForestClassifier(n_estimators=150, max_depth=15,
                                      random_state=RANDOM_SEED, n_jobs=-1, class_weight='balanced')
    rf_model.fit(X_lex_train_scaled, y_train)
    rf_pred_proba = rf_model.predict_proba(X_lex_test_scaled)[:, 1]
    rf_predictions = (rf_pred_proba > 0.5).astype(int)
    print("Random Forest ××•××Ÿ ×•×”×•×¢×¨×š.")

    # ×”. ××™××•×Ÿ ××•×“×œ Deep Learning (×¡×× ×˜×™)
    print("\n--- ××™××•×Ÿ Deep Neural Network (×¡×× ×˜×™) ---")
    nn_model = create_nn_model(EMBEDDING_DIM)

    # ×©×™××•×© ×‘-Early Stopping ×›×“×™ ×œ×× ×•×¢ Overfitting
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    nn_model.fit(X_sem_train, y_train,
                 epochs=20,
                 batch_size=64,
                 validation_data=(X_sem_test, y_test),
                 callbacks=[early_stopping],
                 verbose=0)

    nn_pred_proba = nn_model.predict(X_sem_test).flatten()
    nn_predictions = (nn_pred_proba > 0.5).astype(int)
    print("Neural Network ××•×× ×” ×•×”×•×¢×¨×›×”.")

    # ×•. ×©×™×œ×•×‘ ××•×“×œ×™× (Ensemble/Hybrid - ×”×’×™×©×” ×”××©×•×¤×¨×ª)
    # ××©×§×•×œ×•×ª ×©× ×§×‘×¢×• ×¢×œ ×‘×¡×™×¡ ×”×¢×•×¦××” ×”×¦×¤×•×™×” ×©×œ ×›×œ ×’×™×©×”:
    # ×œ××™×“×” ×¢××•×§×” (BERT) ××§×‘×œ×ª ××©×§×œ ×’×‘×•×” ×™×•×ª×¨ ×¢×œ ×¡××š ×”××××¨ ×”×¨××©×•×Ÿ (PMANet).
    WEIGHT_RF = 0.35
    WEIGHT_NN = 0.65
    print(f"\n--- ×©×™×œ×•×‘ ×”×™×‘×¨×™×“×™: RF={WEIGHT_RF}, NN={WEIGHT_NN} ---")

    ensemble_proba = (rf_pred_proba * WEIGHT_RF) + (nn_pred_proba * WEIGHT_NN)
    ensemble_predictions = (ensemble_proba > 0.5).astype(int)

    # ×–. ×”×¢×¨×›×ª ×‘×™×¦×•×¢×™×
    print("\n" + "=" * 50)
    print("            ğŸ“Š ×¡×™×›×•× ×‘×™×¦×•×¢×™× ×¢×œ × ×ª×•× ×™ ×”×‘×“×™×§×”")
    print("=" * 50)

    print("\n**1. Random Forest (×ª×›×•× ×•×ª ×œ×§×¡×™×§×œ×™×•×ª):**")
    print(classification_report(y_test, rf_predictions, target_names=['Benign (0)', 'Malicious (1)']))

    print("\n**2. Neural Network (×ª×›×•× ×•×ª ×¡×× ×˜×™×•×ª - BERT):**")
    print(classification_report(y_test, nn_predictions, target_names=['Benign (0)', 'Malicious (1)']))

    print("\n**3. ××•×“×œ ×”×™×‘×¨×™×“×™ ××©×•×œ×‘ (×”××©×•×¤×¨):**")
    print(classification_report(y_test, ensemble_predictions, target_names=['Benign (0)', 'Malicious (1)']))
    print(f"×“×™×•×§ ×›×•×œ×œ (Accuracy): {accuracy_score(y_test, ensemble_predictions):.4f}")
    print(f"×¦×™×•×Ÿ F1 ×××•×¦×¢: {f1_score(y_test, ensemble_predictions, average='weighted'):.4f}")
    print("=" * 50)

    # ×—. ×©××™×¨×ª ×”××•×“×œ×™× ×•×”××©×§×•×œ×•×ª
    joblib.dump(rf_model, 'RF_lexical_model.joblib')
    nn_model.save_weights('NN_semantic_weights.h5')

    # ×©××™×¨×ª × ×ª×•× ×™× ×œ×“×•×’××” ×¢×‘×•×¨ ××•×“×œ ×”×”×™×‘×¨×™×“×™
    np.save('ensemble_weights.npy', np.array([WEIGHT_RF, WEIGHT_NN]))

    print("\nâœ… ×”××™××•×Ÿ ×”×¡×ª×™×™× ×‘×”×¦×œ×—×”.")
    print("×”××•×“×œ×™× × ×©××¨×•: RF_lexical_model.joblib, NN_semantic_weights.h5, ensemble_weights.npy, scaler_lexical.joblib")
    print("×›×“×™ ×œ×”×©×ª××© ×‘××•×“×œ ×”××©×•×œ×‘, ×¢×œ×™×š ×œ×˜×¢×•×Ÿ ××ª ×©× ×™×”× ×•××ª ××©×§×•×œ×•×ª ×”×©×™×œ×•×‘.")


if __name__ == "__main__":
    # ×”×’×“×¨×ª ××§×¨××™×•×ª ×œ×ª×•×¦××•×ª ×©×—×–×•×¨
    tf.random.set_seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    train_and_evaluate()