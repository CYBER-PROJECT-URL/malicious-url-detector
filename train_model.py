import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from urllib.parse import urlparse
import re

# --- 1. ×”×’×“×¨×•×ª ×•×˜×¢×™× ×ª × ×ª×•× ×™× (×™×© ×œ×”×ª××™× ××ª ×§×•×‘×¥ ×”× ×ª×•× ×™×) ---
# × ×“×¨×© ×§×•×‘×¥ CSV ×¢× ×¢××•×“×•×ª 'url' (×›×ª×•×‘×ª ×”××ª×¨) ×•-'label' (0 - ×‘×˜×•×—, 1 - ×–×“×•× ×™)
DATASET_PATH = 'malicious_urls_dataset.csv'  # ×©× ×” ×œ× ×ª×™×‘ ×”×××™×ª×™ ×©×œ ×”×§×•×‘×¥ ×©×œ×š
RANDOM_SEED = 42

try:
    data = pd.read_csv(DATASET_PATH)
except FileNotFoundError:
    print(f"×©×’×™××”: ×”×§×•×‘×¥ {DATASET_PATH} ×œ× × ××¦×. ×× × ×•×“× ×©×”× ×ª×™×‘ × ×›×•×Ÿ.")
    exit()

# ×”×¡×¨×ª ×©×•×¨×•×ª ×¢× ×¢×¨×›×™ NULL
data = data.dropna(subset=['url', 'label'])
print(f"×¡×”\"×› ×“×•×’×××•×ª ×œ× ×™×ª×•×—: {len(data)}")


# --- 2. ××™×¦×•×™ ×ª×›×•× ×•×ª ×œ×§×¡×™×§×œ×™×•×ª/××‘× ×™×•×ª (×›××• ×‘××××¨ ×”×©× ×™) ---

def extract_lexical_features(url):
    """××—×œ×¥ ×ª×›×•× ×•×ª ××‘× ×™×•×ª ×•×œ×§×¡×™×§×œ×™×•×ª ××›×ª×•×‘×ª ×”××ª×¨."""
    if not isinstance(url, str):
        return [0] * 9

    features = {}

    # 1. ××•×¨×š URL ×›×•×œ×œ
    features['url_length'] = len(url)

    # 2. × ×•×›×—×•×ª IP ×‘×›×ª×•×‘×ª ×”××ª×¨
    features['has_ip'] = 1 if re.search(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', url) else 0

    # 3. ××¡×¤×¨ ×¡×™×× ×™ '@'
    features['at_sign'] = url.count('@')

    # 4. ××¡×¤×¨ ×¡×™×× ×™ '-'
    features['hyphens'] = url.count('-')

    # 5. ××¡×¤×¨ ×¡×™×× ×™ '/'
    features['slash'] = url.count('/')

    # 6. ××•×¨×š ×”-Hostname
    parsed = urlparse(url)
    features['hostname_length'] = len(parsed.netloc)

    # 7. ×¢×•××§ ×”× ×ª×™×‘ (××¡×¤×¨ ×”×¡××‘-×“×•××™×™× ×™×)
    features['path_depth'] = url.count('//')

    # 8. × ×•×›×—×•×ª ×§×™×¦×•×¨ (×œ××©×œ, 'bit.ly')
    features['is_shortened'] = 1 if ('bit.ly' in url or 'goo.gl' in url) else 0

    # 9. × ×•×›×—×•×ª HTTPS (××™× ×“×™×§×¦×™×” ×œ× ×‘×”×›×¨×— ×‘×˜×•×—×”, ××‘×œ ×©×™××•×©×™×ª)
    features['has_https'] = 1 if url.startswith('https') else 0

    return list(features.values())


# ×”×—×œ×ª ××™×¦×•×™ ×”×ª×›×•× ×•×ª
lexical_features = data['url'].apply(lambda x: pd.Series(extract_lexical_features(x)))
lexical_features.columns = [
    'url_length', 'has_ip', 'at_sign', 'hyphens', 'slash',
    'hostname_length', 'path_depth', 'is_shortened', 'has_https'
]
X_lexical = lexical_features.values
y = data['label'].values

print("×ª×›×•× ×•×ª ×œ×§×¡×™×§×œ×™×•×ª ×—×•×œ×¦×•.")
#

# --- 3. ×”×“××™×™×ª ×™×™×¦×•×’ ×¡×× ×˜×™ (Embedding - ×‘×”×©×¨××ª PMANet) ---
# ×”×¢×¨×”: ×™×¦×™×¨×ª ×™×™×¦×•×’×™ BERT ×‘×¤×•×¢×œ ×“×•×¨×©×ª ×”×ª×§× ×ª ×¡×¤×¨×™×•×ª ×›×‘×“×•×ª (transformers, PyTorch/TensorFlow)
# ×•×–××Ÿ ×—×™×©×•×‘ ××©××¢×•×ª×™. ×œ×¦×•×¨×š ×”×“×’××”, ×× ×• ×™×•×¦×¨×™× ×¢×¨×›×™× ×¨× ×“×•××œ×™×™× ×”××“××™× ×™×™×¦×•×’×™× ××œ×•.
# ×‘×©×™××•×© ×××™×ª×™, ×™×© ×œ×”×—×œ×™×£ ××ª ×”×§×•×“ ×”×–×” ×‘×™×™×¦×•×’ ×××™×ª×™.

EMBEDDING_DIM = 768  # ×××“ ×”×™×™×¦×•×’ ×©×œ BERT
NUM_SAMPLES = len(data)

# ×”×“××™×” ×©×œ ×”×™×™×¦×•×’ ×”×¡×× ×˜×™ (×™×© ×œ×”×—×œ×™×£ ×‘-BERT Embeddings ×××™×ª×™!)
X_semantic = np.random.rand(NUM_SAMPLES, EMBEDDING_DIM)
print(f"×ª×›×•× ×•×ª ×¡×× ×˜×™×•×ª (×”×“××™×”): {X_semantic.shape}")

# --- 4. ×¤×™×¦×•×œ × ×ª×•× ×™× ---
X_lex_train, X_lex_test, y_train, y_test = train_test_split(
    X_lexical, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y
)
X_sem_train, X_sem_test, _, _ = train_test_split(
    X_semantic, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y
)

# --- 5. ××™××•×Ÿ ××•×“×œ ×§×œ××¡×™ (Random Forest - ×›××• ×‘××××¨ ×”×©× ×™) ---
print("\n--- ××™××•×Ÿ Random Forest ---")
rf_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1)
rf_model.fit(X_lex_train, y_train)

# --- 6. ××™××•×Ÿ ××•×“×œ ×¨×©×ª × ×•×™×¨×•× ×™× (Deep Learning - ×‘×”×©×¨××ª PMANet) ---
# ××•×“×œ ×¨×©×ª × ×•×™×¨×•× ×™× ×¤×©×•×˜ (×‘××§×•× ×¨×©×ª ×”×§×©×‘ ×”××•×¨×›×‘×ª ×©×œ PMANet)
print("\n--- ××™××•×Ÿ ×¨×©×ª × ×•×™×¨×•× ×™× (Deep Learning) ---")

nn_model = Sequential([
    Dense(512, activation='relu', input_shape=(EMBEDDING_DIM,)),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')  # ×™×¦×™××” ×‘×™× ××¨×™×ª
])

nn_model.compile(optimizer=Adam(learning_rate=0.001),
                 loss='binary_crossentropy',
                 metrics=['accuracy'])

# ××™××•×Ÿ ×”×¨×©×ª ×”× ×•×™×¨×•× ×™×ª ×¢×œ ×”×ª×›×•× ×•×ª ×”×¡×× ×˜×™×•×ª
nn_model.fit(X_sem_train, y_train, epochs=5, batch_size=32, verbose=0)
print("××™××•×Ÿ ×¨×©×ª × ×•×™×¨×•× ×™× ×”×¡×ª×™×™×.")

# --- 7. ×©×™×œ×•×‘ ××•×“×œ×™× (Ensemble - ×©×™×¤×•×¨ ××©×•×œ×‘) ---
print("\n--- ×©×™×œ×•×‘ ××•×“×œ×™× (Voting Classifier) ---")

# ×™×¦×™×¨×ª ×ª×—×–×™×•×ª ×”×¡×ª×‘×¨×•×ª
rf_proba = rf_model.predict_proba(X_lex_test)[:, 1]
nn_proba = nn_model.predict(X_sem_test).flatten()

# ×©×™×œ×•×‘ ×”×ª×—×–×™×•×ª ×‘×××¦×¢×•×ª ×××•×¦×¢ ××©×•×§×œ×œ (× ×™×ª×Ÿ ×œ×”×ª××™× ××©×§×œ×™×)
WEIGHT_RF = 0.4
WEIGHT_NN = 0.6
ensemble_proba = (rf_proba * WEIGHT_RF) + (nn_proba * WEIGHT_NN)
ensemble_predictions = (ensemble_proba > 0.5).astype(int)


# --- 8. ×”×¢×¨×›×ª ×‘×™×¦×•×¢×™× ---

## ğŸ“Š ×”×¢×¨×›×ª ×‘×™×¦×•×¢×™×
def evaluate_model(y_true, y_pred, model_name):
    """××“×¤×™×¡ ××“×“×™ ×‘×™×¦×•×¢×™×."""
    print(f"\n### {model_name} ###")
    print(f"×“×™×•×§ (Accuracy): {accuracy_score(y_true, y_pred):.4f}")
    print(f"×“×™×•×§ ×—×™×•×‘×™ (Precision): {precision_score(y_true, y_pred):.4f}")
    print(f"×›×™×¡×•×™ (Recall): {recall_score(y_true, y_pred):.4f}")
    print(f"×¦×™×•×Ÿ F1: {f1_score(y_true, y_pred):.4f}")


# ×”×¢×¨×›×ª Random Forest (×ª×›×•× ×•×ª ×œ×§×¡×™×§×œ×™×•×ª)
rf_predictions = rf_model.predict(X_lex_test)
evaluate_model(y_test, rf_predictions, "Random Forest (×œ×§×¡×™×§×œ×™)")

# ×”×¢×¨×›×ª ×¨×©×ª × ×•×™×¨×•× ×™× (×ª×›×•× ×•×ª ×¡×× ×˜×™×•×ª)
nn_predictions = (nn_model.predict(X_sem_test).flatten() > 0.5).astype(int)
evaluate_model(y_test, nn_predictions, "Neural Network (×¡×× ×˜×™)")

# ×”×¢×¨×›×ª ×”××•×“×œ ×”××©×•×œ×‘ (×”×™×‘×¨×™×“×™) - ×”×’×™×©×” ×”××©×•×¤×¨×ª
evaluate_model(y_test, ensemble_predictions, "××•×“×œ ××©×•×œ×‘ (×”×™×‘×¨×™×“×™)")

# --- 9. ×©××™×¨×ª ×”××•×“×œ×™× (Random Forest ×•×©××™×¨×ª ××©×§×•×œ×•×ª ×”×¨×©×ª ×”× ×•×™×¨×•× ×™×ª) ---
import joblib

# ×©××™×¨×ª ××•×“×œ Random Forest
JOB_LIB_PATH = 'random_forest_model.joblib'
joblib.dump(rf_model, JOB_LIB_PATH)
print(f"\n××•×“×œ Random Forest × ×©××¨ ×‘: {JOB_LIB_PATH}")

# ×©××™×¨×ª ××©×§×•×œ×•×ª ×”×¨×©×ª ×”× ×•×™×¨×•× ×™×ª
H5_PATH = 'neural_network_weights.h5'
nn_model.save_weights(H5_PATH)
print(f"××©×§×•×œ×•×ª ×”×¨×©×ª ×”× ×•×™×¨×•× ×™×ª × ×©××¨×• ×‘: {H5_PATH}")

print("\nâœ… ×¡×™×•× ×”××™××•×Ÿ. × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘××•×“×œ×™× ×©× ×©××¨×• ×œ×§×‘×œ×ª ×ª×—×–×™×•×ª.")

# --- ×¡×•×£ ×”×§×•×“ ---